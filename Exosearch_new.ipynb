{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9134997-c9e8-43db-acbf-1f4807dfb447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juliajos/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/juliajos/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/juliajos/.local/lib/python3.9/site-packages/lightkurve/prf/__init__.py:7: UserWarning: Warning: the tpfmodel submodule is not available without oktopus installed, which requires a current version of autograd. See #1452 for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#util \n",
    "import os\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from lightkurve.correctors import load_tess_cbvs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import lightkurve as lk\n",
    "from info import cadence_bounds\n",
    "\n",
    "\n",
    "def safe_div(n, d):\n",
    "    return n / d if d else 0\n",
    "\n",
    "def load_cbv(sector: int, camera: int, ccd: int, directory: str = \".\"):\n",
    "    sector_str = f\"{sector:04d}\"\n",
    "    # Find filename pattern\n",
    "    for fname in os.listdir(directory):\n",
    "        if (\n",
    "            f\"-s{sector_str}-\" in fname\n",
    "            and f\"{camera}-{ccd}\" in fname\n",
    "            and fname.endswith(\"_cbv.fits\")\n",
    "        ):\n",
    "            filepath = os.path.join(directory, fname)\n",
    "            print(f\"Opening: {filepath}\")\n",
    "            with fits.open(filepath) as hdul:\n",
    "                N_vecs = 0\n",
    "                cbv_matrix = []\n",
    "                cadence = CADENCENO\n",
    "                with fits.open(dir+'cbv_sector'+str(sector)+'/'+cbv_names[sector-1] % (1, 1), memmap=True) as hdulist2:\n",
    "                    for j in range(30):\n",
    "                        k = j+1\n",
    "                        try:\n",
    "                            evec = hdulist2[1].data['VECTOR_%s' % k]\n",
    "                            cbv_matrix.append(evec)\n",
    "                            if np.any(evec): N_vecs += 1\n",
    "                        except: continue\n",
    "                        raise FileNotFoundError(f\"No CBV FITS found for sector {sector}, cam {camera}, ccd {ccd}\")\n",
    "    return cbv_matrix, cadence, N_vecs\n",
    "\n",
    "# Example usage:\n",
    "# cbv_matrix, cadence, N_vec = load_cbv(sector=70, camera=4, ccd=4)\n",
    "\n",
    "def check_symmetric(a, rtol=1e-05):\n",
    "    return (np.sum(a-a.T) < rtol)\n",
    "\n",
    "def median_normal(lightcurve):\n",
    "    \"\"\"\n",
    "    Median normalize lightcurve\n",
    "    \n",
    "    Args:\n",
    "    lightcurve : lightcurve to be normalized.\n",
    "        \n",
    "    Returns:\n",
    "    Median-normalized lightcurve.\n",
    "    \"\"\"\n",
    "    lightcurve -= np.nanmedian(lightcurve)\n",
    "    return lightcurve / np.nanmedian(np.abs(lightcurve))\n",
    "\n",
    "def mag_normal_med(lightcurve):\n",
    "    \"\"\"\n",
    "    Normalize lightcurve by magnitude (to be used if calculating pairwise correlation with corr_comp)\n",
    "    \n",
    "    Args:\n",
    "    lightcurve :lightcurve to be normalized.\n",
    "        \n",
    "    Returns:\n",
    "    magnitude-normalized data.\n",
    "    \"\"\"\n",
    "    lightcurve -= np.nanmedian(lightcurve) #Subtract median as slightly more robust to outliers\n",
    "    return lightcurve / np.linalg.norm(lightcurve)\n",
    "\n",
    "def mag_normal_mean(lightcurve):\n",
    "    \"\"\"\n",
    "    Normalize lightcurve by magnitude (to be used if calculating pairwise correlation with corr_comp)\n",
    "    \n",
    "    Args:\n",
    "    lightcurve :lightcurve to be normalized.\n",
    "        \n",
    "    Returns:\n",
    "    magnitude-normalized data.\n",
    "    \"\"\"\n",
    "    lightcurve -= np.nanmean(lightcurve)\n",
    "    return lightcurve / np.linalg.norm(lightcurve)\n",
    "\n",
    "def calc_CDPP(lightcurve, scale, offset):\n",
    "    \"\"\"\n",
    "    CDPP in PPM\n",
    "    \"\"\"\n",
    "    lightcurve *= scale\n",
    "    smooth_reg = lightcurve - savgol_filter(lightcurve, 97, 2)\n",
    "    smooth_reg = threshold_data(smooth_reg)\n",
    "    mean_bin = np.zeros(len(smooth_reg)-14)\n",
    "    for j in range(len(mean_bin)):\n",
    "        mean_bin[j] = np.mean(smooth_reg[j:j+13])\n",
    "    cdpp_reg = ( np.std(mean_bin)*1.168 / np.median(offset) ) / (1e-6) # CDPP in PPM\n",
    "    return cdpp_reg\n",
    "\n",
    "def linear_detrend(lightcurve):\n",
    "    \"\"\"\n",
    "    Linearly detrend an individual lightcurve\n",
    "\n",
    "    Args:\n",
    "    lightcurve : Length N time series\n",
    "    \n",
    "    Returns:\n",
    "    lightcurve with linear trend removed\n",
    "    \n",
    "    \"\"\"\n",
    "    z = np.polyfit(range(len(lightcurve)), lightcurve, 1)\n",
    "    p = np.poly1d(z)\n",
    "    return lightcurve - p(range(len(lightcurve)))\n",
    "\n",
    "def threshold_data(data, base_data = None, level=4):\n",
    "    \"\"\"\n",
    "    Threshold outliers (flux samples) at level*std dev and replace with Gaussian random samples.\n",
    "    \n",
    "    The filtering is performed in a two-step procedure by first applying a coarse threshold\n",
    "    to remove extremal values and using this data to calculate the std dev. \n",
    "    \n",
    "    Args:\n",
    "    data : 1D array containing the data to be thresholded.\n",
    "    base_data (optional) : base the calculation of points to be thresholded on base_data if supplied (thresholding applied to data)\n",
    "    level (optional) : Factor by which the standard deviation is multiplied to set the threshold level. Default is 5.\n",
    "        \n",
    "    Returns:\n",
    "    1D array containing the thresholded data.\n",
    "        \n",
    "    \"\"\"\n",
    "    if base_data is None: base_data = data\n",
    "    std_ = np.nanstd(base_data)\n",
    "    diff = np.diff(base_data, prepend=base_data[0])\n",
    "    thresh = level*std_\n",
    "    mask = np.ones(len(base_data), dtype=bool)\n",
    "\n",
    "    mask[np.abs(base_data) > thresh] = False\n",
    "    mask[np.abs(diff) > thresh] = False\n",
    "\n",
    "    std_clean = np.nanstd(base_data[mask])\n",
    "    thresh = level*std_clean\n",
    "\n",
    "    mask = np.zeros(len(data), dtype=bool)    \n",
    "    mask[np.abs(base_data) > thresh] = True\n",
    "    mask[np.abs(diff) > thresh] = True\n",
    "\n",
    "    data[mask] = np.random.normal(0, std_clean, size=mask.sum())\n",
    "    return data\n",
    "\n",
    "def nan_linear_gapfill(data):\n",
    "    \"\"\"\n",
    "    Fill NaN gaps in data using linear interpolation.\n",
    "    \n",
    "    The function identifies groups of consecutive NaNs in the data and fills them using \n",
    "    a linear interpolation approach based on the values immediately adjacent to the gaps.\n",
    "    \n",
    "    Args:\n",
    "    data : 1D array containing the data with NaN gaps to be filled.\n",
    "        \n",
    "    Returns:\n",
    "    1D array where NaN gaps have been filled using linear interpolation.\n",
    "        \n",
    "    \"\"\"\n",
    "    goodind = np.where(~np.isnan(data))\n",
    "    badind = np.where(np.isnan(data))\n",
    "    gaps = [list(group) for group in mit.consecutive_groups(badind[0])]\n",
    "    for g in gaps:\n",
    "        if len(g) == 1:\n",
    "            data[g[0]] = data[g[0]-1]\n",
    "            continue\n",
    "        else:\n",
    "            grad = (data[g[len(g)-1]+1]-data[g[0]-1])/(len(g)+2)\n",
    "            data[g] = (np.arange(len(g))*grad) + data[g[0]-1]\n",
    "    return data\n",
    "\n",
    "\n",
    "def cbv_matrix(lc_cadence, sector, cam, ccd, model_order = None):\n",
    "    '''\n",
    "    Load and mask the TESS cotrending basis vectors (otherwise fit own basis)\n",
    "    '''\n",
    "    cbvs = load_tess_cbvs(sector=sector, camera=cam, ccd=ccd, cbv_type='SingleScale')\n",
    "    if model_order == None: model_order = 16\n",
    "    cbv_dm = cbvs.to_designmatrix(cbv_indices=np.arange(1, model_order+1))\n",
    "    V = cbv_dm.values\n",
    "    V_masked = V[np.in1d(cbvs.cadenceno, lc_cadence)]\n",
    "    return V_masked.T\n",
    "\n",
    "def covariance_stellar(lc, cadence_data, N_cadence):\n",
    "    '''\n",
    "    Estimate stellar covariance model (stationary toeplitz model), using spectral estimator on detrended light curve\n",
    "    '''\n",
    "    filled_lc = np.zeros(N_cadence)\n",
    "    mask = np.zeros(N_cadence, dtype=bool)\n",
    "    mask[cadence_data] = True\n",
    "\n",
    "    filled_lc[mask] = threshold_data(lc, level=3)\n",
    "    std_ = np.std(filled_lc[cadence_data])\n",
    "    filled_lc[~mask] = np.random.normal(0, std_, N_cadence - len(cadence_data))\n",
    "\n",
    "    zp_lc = np.zeros((2*N_cadence) - 1)\n",
    "    zp_lc[:N_cadence] = filled_lc\n",
    "    p_noise_smooth = smooth_p(zp_lc, K=3)\n",
    "    ac = np.real(np.fft.ifft(p_noise_smooth)).astype('float32')\n",
    "    ac = ac[:N_cadence]\n",
    "    \n",
    "    cov_stellar = toeplitz(ac, r = ac)\n",
    "    masked_cov_stellar = cov_stellar[cadence_data]\n",
    "    masked_cov_stellar = masked_cov_stellar[:, cadence_data]\n",
    "    return cov_stellar, masked_cov_stellar\n",
    "    \n",
    "def covariance_sector(tid, sector):\n",
    "    (lc_data, processed_lc_data, detrend_data, norm_offset, quality_data, time_data, cam_data, ccd_data, coeff_ls, centroid_xy_data, pos_xy_corr) = pickle.load(open(os.path.expanduser('TESS/data/light_curves/%s.p' % (tic_id)), 'rb')) \n",
    "    cov_c = pickle.load( open(f\"TESS/data/priors/{sector}/cov_c%s_%s_%s.p\" % (sector, cam, ccd), \"rb\" ))\n",
    "    lc_cadence_zero =  time_data[sector] - cadence_bounds[sector][0] - 1\n",
    "    N_cadence = cadence_bounds[sector][1]-cadence_bounds[sector][0]\n",
    "    _, cov_s = covariance_stellar(detrend_data[sector].unmasked, cadence_data = lc_cadence_zero, N_cadence = N_cadence)\n",
    "    V = pickle.load(open(f\"TESS/data/priors/{sector}/evec_matrix_%s_%s_%s.p\" % (sector, cam_data[sector], ccd_data[sector]), \"rb\" )) #add correct path!\n",
    "    V = V[:, lc_cadence_zero]\n",
    "    cov_z = np.dot(V.T, np.dot(cov_c, V)) + cov_s\n",
    "    cov_inv_z = jax.numpy.linalg.pinv(cov_z)    \n",
    "    #cov_inv_z = np.linalg.inv(cov_z)\n",
    "\n",
    "    print ('symmetry check', check_symmetric(cov_inv_z))\n",
    "    print ('nan check',  np.sum(np.isnan(cov_inv_z)))\n",
    "    print (np.sum(cov_inv_z))\n",
    "    \n",
    "    lc_detrend_full = np.zeros(N_cadence)\n",
    "    lc_detrend_full[lc_cadence_zero] = detrend_data[sector]\n",
    "    cov_inv_z_full = np.zeros((N_cadence, N_cadence))\n",
    "    cov_inv_z_full[np.ix_(lc_cadence_zero, lc_cadence_zero)] = cov_inv_z\n",
    "    return lc_detrend_full, cov_inv_z_full\n",
    "    \n",
    "def covariance_model(lc, lc_cadence, sector, cam, ccd, model_order = None, full=True):\n",
    "    '''\n",
    "    H1: y = z + t + n, H0: y = z + n\n",
    "    z ~ N(V*mu_c, V cov_c V.T + cov_*)\n",
    "    Estimate joint covariance of z: Cov_stellar (stationary) + Cov_systematics (low rank)\n",
    "\n",
    "    args\n",
    "    model_order : systematics model order\n",
    "    full: return the covariance and light curve, with uniform time sampling and zero's at masked/missing samples\n",
    "    '''\n",
    "    V = cbv_matrix(lc_cadence, sector, cam, ccd, model_order = model_order)\n",
    "    ls_fit = np.dot(V, lc.T).dot(V) \n",
    "    lc_detrend = lc - ls_fit\n",
    "\n",
    "    # load the systematic noise covariance (estimated from collection of light curves on the same sensor)\n",
    "    #cov_c = pickle.load( open(\"cov_c_diag%s_%s_%s.p\" % (sector, cam, ccd), \"rb\" ))\n",
    "    cov_c = pickle.load( open(\"cov_c%s_%s_%s.p\" % (sector, cam, ccd), \"rb\" ))\n",
    "\n",
    "    cov_s = covariance_stellar(np.copy(lc_detrend), lc_cadence)\n",
    "\n",
    "    cov_z = np.dot(V.T, np.dot(cov_c, V)) + cov_s\n",
    "    #cov_inv_z = jax.numpy.linalg.pinv(cov_z)\n",
    "    \n",
    "    cov_inv_z = np.linalg.inv(cov_z)\n",
    "\n",
    "    print ('symmetry check', check_symmetric(cov_inv_z))\n",
    "    print ('nan check',  np.sum(np.isnan(cov_inv_z)))\n",
    "    print (np.sum(cov_inv_z))\n",
    "    if full:\n",
    "        lc_cadence_zero = lc_cadence - lc_cadence[0]\n",
    "        cadence_len = lc_cadence_zero[-1]\n",
    "        lc_detrend_full = np.zeros(cadence_len+1)\n",
    "        lc_detrend_full[lc_cadence_zero] = lc_detrend\n",
    "        cov_inv_z_full = np.zeros((cadence_len+1, cadence_len+1))\n",
    "        cov_inv_z_full[np.ix_(lc_cadence_zero, lc_cadence_zero)] = cov_inv_z\n",
    "        print (np.sum(cov_inv_z_full))\n",
    "        return lc_detrend_full, cov_inv_z_full\n",
    "    else:\n",
    "        return lc_detrend, cov_inv_z\n",
    "\n",
    "def smooth_p(noise, K=3):\n",
    "    '''\n",
    "    Computes the smoothed periodogram\n",
    "    '''\n",
    "    N = len(noise)\n",
    "    p_noise = (1/float(N)) * (np.abs(np.fft.fft(noise))**2)\n",
    "    integ_periodogram = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if i<K: integ_periodogram[i] = np.sum(p_noise[:i+K])\n",
    "        elif i>N-K: integ_periodogram[i] = np.sum(p_noise[i-K:])\n",
    "        else: integ_periodogram[i] = np.sum(p_noise[i-K:i+K])\n",
    "    integ_periodogram *= (1/float(2*K))\n",
    "    return integ_periodogram\n",
    "\n",
    "def box_transit(times_, period, dur, t0, alpha=1):\n",
    "    \"\"\"\n",
    "    Generate a transit signal time-series with box function evaluated at given times.\n",
    "    \n",
    "    Args:\n",
    "    times_ :  Array of time points at which to evaluate the transit time-series\n",
    "    period : Period of the transit\n",
    "    dur : Duration of the transit.\n",
    "    t0 : Epoch.\n",
    "    alpha : Transit depth. Default is 1.\n",
    "        \n",
    "    Returns:\n",
    "    Transit time series evaluated at `times_`.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.piecewise(times_, [((times_-t0+(dur/2))%period) > dur, ((times_-t0+(dur/2))%period) <= dur], [0, 1])*(-alpha)\n",
    "\n",
    "def nd_argsort(x):\n",
    "    return np.array(np.unravel_index(np.argsort(x, axis=None), x.shape)).T[::-1]\n",
    "\n",
    "\n",
    "def covariance_sector_test(tic_id, sector):\n",
    "    (lc_data, processed_lc_data, detrend_data, norm_offset, quality_data, time_data, cam_data, ccd_data, coeff_ls, centroid_xy_data, pos_xy_corr) = pickle.load(open('TESS/data/light_curves/%s.p' % (tic_id), 'rb')) \n",
    "    lc_cadence_zero =  time_data[sector] - cadence_bounds[sector][0] - 1\n",
    "    N_cadence = cadence_bounds[sector][1]-cadence_bounds[sector][0]\n",
    "\n",
    "    _, cov_s = covariance_stellar(detrend_data[sector].unmasked, cadence_data = lc_cadence_zero, N_cadence = N_cadence)\n",
    "    cov_inv_s = jax.numpy.linalg.pinv(cov_s)    \n",
    "    #cov_inv_s = np.linalg.pinv(cov_s)    \n",
    "\n",
    "    print ('symmetry check', check_symmetric(cov_inv_s))\n",
    "    print ('nan check',  np.sum(np.isnan(cov_inv_s)))\n",
    "    print (np.sum(cov_inv_s))\n",
    "\n",
    "    lc_detrend_full = np.zeros(N_cadence)\n",
    "    lc_detrend_full[lc_cadence_zero] = detrend_data[sector].unmasked\n",
    "    \n",
    "    cov_inv_s_full = np.zeros((N_cadence, N_cadence))\n",
    "    cov_inv_s_full[np.ix_(lc_cadence_zero, lc_cadence_zero)] = cov_inv_s\n",
    "    return lc_detrend_full, cov_inv_s_full\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352b2e4e-132e-445f-8d03-b3e1eb818975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nfrom numpy.polynomial.polynomial import Polynomial\\nfrom info import cadence_bounds\\n\\n# Identify gap indices (likely downlinks)\\ngap_threshold = 360 #1/2 day\\ndt = np.diff(cadence_data)\\ngap_indices = np.where(dt > gap_threshold)[0]\\n\\n# Include start and end for segmentation\\nsplit_indices = np.concatenate(([0], gap_indices + 1, [len(time)]))\\n\\n# Detrend each segment\\nflux_detrended = np.zeros_like(flux)\\nfor i in range(len(split_indices) - 1):\\n    start, end = split_indices[i], split_indices[i+1]\\n    start = np.max(split_indices[i], split_indices[i+1] - 360)\\n    t_seg = time[start:end]\\n    f_seg = flux[start:end]\\n\\n    # Remove NaNs\\n    valid = np.isfinite(t_seg) & np.isfinite(f_seg)\\n    if np.count_nonzero(valid) < 5:\\n        flux_detrended[start:end] = f_seg  # too few points to fit\\n        continue\\n\\n    t_valid = t_seg[valid]\\n    f_valid = f_seg[valid]\\n\\n    # Fit polynomial (degree can be adjusted)\\n    coeffs = Polynomial.fit(t_valid, f_valid, deg=2).convert().coef\\n    trend = Polynomial(coeffs)(t_seg)\\n    flux_detrended[start:end] = f_seg - trend\\n\\n# Replace lc.flux with detrended result\\nlc_detrended = lc.copy()\\nlc_detrended.flux = flux_detrended\\n\\n# Optionally: plot\\nlc_detrended.plot()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#polynomial_detrend\n",
    "'''\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from info import cadence_bounds\n",
    "\n",
    "# Identify gap indices (likely downlinks)\n",
    "gap_threshold = 360 #1/2 day\n",
    "dt = np.diff(cadence_data)\n",
    "gap_indices = np.where(dt > gap_threshold)[0]\n",
    "\n",
    "# Include start and end for segmentation\n",
    "split_indices = np.concatenate(([0], gap_indices + 1, [len(time)]))\n",
    "\n",
    "# Detrend each segment\n",
    "flux_detrended = np.zeros_like(flux)\n",
    "for i in range(len(split_indices) - 1):\n",
    "    start, end = split_indices[i], split_indices[i+1]\n",
    "    start = np.max(split_indices[i], split_indices[i+1] - 360)\n",
    "    t_seg = time[start:end]\n",
    "    f_seg = flux[start:end]\n",
    "\n",
    "    # Remove NaNs\n",
    "    valid = np.isfinite(t_seg) & np.isfinite(f_seg)\n",
    "    if np.count_nonzero(valid) < 5:\n",
    "        flux_detrended[start:end] = f_seg  # too few points to fit\n",
    "        continue\n",
    "\n",
    "    t_valid = t_seg[valid]\n",
    "    f_valid = f_seg[valid]\n",
    "\n",
    "    # Fit polynomial (degree can be adjusted)\n",
    "    coeffs = Polynomial.fit(t_valid, f_valid, deg=2).convert().coef\n",
    "    trend = Polynomial(coeffs)(t_seg)\n",
    "    flux_detrended[start:end] = f_seg - trend\n",
    "\n",
    "# Replace lc.flux with detrended result\n",
    "lc_detrended = lc.copy()\n",
    "lc_detrended.flux = flux_detrended\n",
    "\n",
    "# Optionally: plot\n",
    "lc_detrended.plot()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e2734d-aeba-4154-a3b8-f8af1051eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error fix\n",
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb39a10-c81b-4ce0-b03d-e18f232e5fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symmetry check True\n",
      "nan check 0\n",
      "816.24646\n",
      "(1, 301)\n",
      "Line 53 finished\n",
      "Complete\n",
      "starting loop\n",
      "0\n",
      "ended loop\n",
      "[  0 504 466] 29.484617233276367\n",
      "LRT (SNR):  29.48 duration (hr):  0.0 period (day):  7.01 epoch(day):  6.47\n",
      "[  0 501 466] 25.409574508666992\n",
      "LRT (SNR):  25.41 duration (hr):  0.0 period (day):  6.97 epoch(day):  6.47\n",
      "[  0 519 466] 21.948177337646484\n",
      "LRT (SNR):  21.95 duration (hr):  0.0 period (day):  7.22 epoch(day):  6.47\n",
      "[  0 684 466] 19.71776008605957\n",
      "LRT (SNR):  19.72 duration (hr):  0.0 period (day):  9.51 epoch(day):  6.47\n",
      "[  0 515 466] 18.077341079711914\n",
      "LRT (SNR):  18.08 duration (hr):  0.0 period (day):  7.17 epoch(day):  6.47\n",
      "Script Finished\n"
     ]
    }
   ],
   "source": [
    "#exosearch \n",
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from util import *\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "day_to_cadence = 720\n",
    "\n",
    "def transit_num(y_d, num_period):\n",
    "    ''''\n",
    "    Arg:\n",
    "    y_d : y.Cov_inv * transit_profile_d the size of this is ~ N_full / delta\n",
    "\n",
    "    Returns:\n",
    "    num_det: returns numerator of likelihoods as a 2D array indexed by period/delta and epoch/delta\n",
    "    '''\n",
    "    num_det = jnp.zeros((num_period, num_period)) \n",
    "    for p in range(num_period):\n",
    "        for t0 in range(p+1):\n",
    "            num_det = num_det.at[p, t0].set(jnp.sum(y_d[t0 :: p+1]))\n",
    "    return num_det\n",
    "\n",
    "def transit_den(K_d, num_period):\n",
    "    '''\n",
    "    Arg:\n",
    "    K_d : Cov_inv * (transit_profile_d.transit_profile_d^T) the size of this is ~ (N_full / delta, N_full / delta)\n",
    "\n",
    "    Returns:\n",
    "    den_det: returns denominator of likelihoods as a 2D array indexed by period/delta and epoch/delta\n",
    "    '''\n",
    "    den_det = jnp.zeros((num_period, num_period))\n",
    "    for p in range(num_period):\n",
    "        for t0 in range(p+1): \n",
    "            den_det = den_det.at[p, t0].set(jnp.sum(K_d[t0 :: p+1, t0 :: p+1]))\n",
    "    return den_det\n",
    "\n",
    "\n",
    "# Load lc and inverse covariance model\n",
    "# ====================================\n",
    "#(lc_data, processed_lc_data, detrend_data, norm_offset, quality_data, time_data, cam_data, ccd_data, coeff_ls, centroid_xy_data, pos_xy_corr) = pickle.load(open(os.path.expanduser('~/TESS/data/%s.p' % (tic_id)), 'rb')) \n",
    "\n",
    "#our own list of TIC ID's to search\n",
    "\n",
    "df = pd.read_csv('persistant_tids.txt', header=None, names=['tic_id'])\n",
    "tic_ids = df['tic_id'].tolist() #reads out our list of tids\n",
    "\n",
    "\n",
    "tic_id = tic_ids[504]\n",
    "sector = 73\n",
    "cam = 1\n",
    "ccd = 1\n",
    "\n",
    "lc_detrend, cov_inv = covariance_sector(tic_id, sector)\n",
    "#data = np.load('TESS/data/light_curves/info/transit_templates(2).npz', allow_pickle=True)\n",
    "#transit_profile_d = data['transit_templates']\n",
    "#transit_profile_d = pickle.load(open(\"TESS/data/light_curves/info/transit_templates.p\", \"rb\"))\n",
    "data = np.load(\"transit_templates (2).npz\")\n",
    "print (data['10'].shape)\n",
    "print(\"Line 53 finished\")\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Defining transit parameter search space (period, epoch, duration)\n",
    "# Period ranges from 0 to N/2\n",
    "# epoch ranges from 0 to P\n",
    "\n",
    "delta = 10 # period and epoch step size in 2-minute samples (origianlly 5)\n",
    "durations = jnp.array([1, 2, 3, 4, 6, 8, 10, 12, 14, 16])*30 \n",
    "N_full = len(lc_detrend)\n",
    "lc_cov_inv = cov_inv.dot(lc_detrend) # y^T Cov_inv\n",
    "# Compute transit likelihoods over parameter space \n",
    "\n",
    "print(\"Complete\")\n",
    "\n",
    "num_period = int((N_full - durations[-1]) // (2 * delta))# number of periods to search in stepsize of delta\n",
    "transit_likelihood_stats = np.zeros((len(durations), num_period, num_period))\n",
    "\n",
    "jit_lik_num = jax.jit(transit_num, static_argnums=(1))\n",
    "jit_lik_den = jax.jit(transit_den, static_argnums=(1))\n",
    "\n",
    "print(\"starting loop\")\n",
    "\n",
    "for i in range(len(durations)):\n",
    "    print (i)\n",
    "    d = durations[i]\n",
    "    #transit_profile = jnp.ones(d) * transit_profile_d[durations[i]//30][:d]\n",
    "    transit_profile = jnp.ones(d) #* transit_profile_d[durations[i]//30][:d] \n",
    "    transit_kernel = jnp.outer(transit_profile, transit_profile)\n",
    "\n",
    "    #y_d = jax.scipy.signal.convolve(lc_cov_inv, transit_profile)[int(d/2)-1:N_full-int(d/2)-1][::delta]\n",
    "    y_d = jax.scipy.signal.convolve(lc_cov_inv, transit_profile, method='fft')[int(d/2)-1:N_full-int(d/2)-1][::delta]\n",
    "    \n",
    "    \n",
    "    # commented this out as I get a memory error\n",
    "    #K_d = jax.scipy.signal.convolve2d(cov_inv, transit_kernel)[int(d/2)-1:N_full-int(d/2)-1,int(d/2)-1:N_full-int(d/2)-1][::delta,::delta]\n",
    "\n",
    "    # different way to calculate K_d\n",
    "    K_d = np.zeros((np.shape(y_d)[0], np.shape(y_d)[0]))\n",
    "    for l in range(num_period):\n",
    "        for m in range(num_period):\n",
    "            K_d[l,m] = np.sum(transit_kernel*cov_inv[(l*delta):(l*delta) + d, (m*delta):(m*delta) + d])    \n",
    "    K_d = jnp.array(K_d)\n",
    "\n",
    "    likelihoods_num = transit_num(y_d, num_period)\n",
    "    likelihoods_den = transit_den(K_d, num_period)\n",
    "\n",
    "    # Output transit detection tests, indexed as [P/delta, t_0/delta]\n",
    "    transit_likelihood_stats[i] = np.divide(likelihoods_num, np.sqrt(likelihoods_den), out=np.zeros_like(likelihoods_num), where=likelihoods_den!=0.)\n",
    "    \n",
    "    break\n",
    "    \n",
    "print(\"ended loop\")\n",
    "    \n",
    "top_detections = nd_argsort(transit_likelihood_stats)\n",
    "\n",
    "for i in range(5):\n",
    "    print (top_detections[i], transit_likelihood_stats[top_detections[i][0], top_detections[i][1], top_detections[i][2]])\n",
    "    print ('LRT (SNR): ', np.round(transit_likelihood_stats[top_detections[i][0], top_detections[i][1], top_detections[i][2]],2), 'duration (hr): ', np.round(top_detections[i][0]/30, 2), 'period (day): ', np.round(delta*(top_detections[i][1]+1)/(day_to_cadence),2), 'epoch(day): ',  np.round((delta/day_to_cadence)*top_detections[i][2], 2) )\n",
    "    \n",
    "print(\"Script Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f9eb89-40ba-4311-84b1-51169e1cc051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d3853ad-4eeb-4cf8-af04-8cf7e5e28e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transit_templates']\n",
      "transit_templates\n"
     ]
    }
   ],
   "source": [
    "data = np.load('TESS/data/light_curves/info/transit_templates.npz', allow_pickle=True)\n",
    "print(data.files)\n",
    "#transit_profile_d = data['transit_templates']\n",
    "#print(transit_profile_d.files)\n",
    "\n",
    "names = data.files\n",
    "print(names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323d9af-9488-42af-8c8b-9cf51fd3e016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a1edbd-56f1-487b-b632-53f0c460f662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 16] Device or resource busy: '.nfs0000001112deafd500000c65'\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user numpy==1.26.4\n",
    "# old version 1.22.4\n",
    "import numpy\n",
    "print(numpy.__version__)\n",
    "print(numpy.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb911a8-5430-4d9e-95b9-3f1a1031c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cuda(id=0), cuda(id=1)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print (jax.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae1e66-16e4-48c9-a31e-d1684debcc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4af91-6a16-40aa-8e72-b7e1b05012c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
